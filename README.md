Hereâ€™s a **clean, professional, interview-ready `README.md`** tailored exactly to your project and repo name.
You can **copy-paste this directly** into GitHub.

---

````md
# ğŸ§  RAG-Based Psychology Mentor AI

A **Retrieval-Augmented Generation (RAG)** based psychology mentor AI that delivers **empathetic, evidence-grounded guidance** from curated psychology literature, featuring **relevance filtering, ethical guardrails, and crisis interception mechanisms**.

> âš ï¸ **Disclaimer:** This project is for educational and research purposes only.  
> It is **not a therapist, psychologist, or medical system** and does **not provide diagnosis or treatment**.

---

## ğŸ“Œ Overview

This project explores how Large Language Models (LLMs) can be safely applied in **psychology-adjacent domains** without crossing ethical or clinical boundaries.

Instead of acting as a therapist, the system functions as a **college-style mentor**:
- translating psychology concepts into **supportive, human-centered language**
- grounding responses in **retrieved academic sources**
- explicitly avoiding diagnosis, medical claims, or dependency

---

## âœ¨ Key Features

- **Retrieval-Augmented Generation (RAG)** using ChromaDB  
- **Empathy-aware prompt design** (mentor tone, not clinical)
- **Context relevance filtering** to reduce hallucinations
- **Crisis keyword interception** with safe redirection
- **Explicit ethical boundaries** (non-therapeutic, non-diagnostic)
- **Short-term conversational memory** to prevent over-dependence
- **Dual interfaces**
  - CLI chatbot
  - Streamlit web application
- **Offline local LLM support** via Ollama (Mistral)

---

## ğŸ—ï¸ System Architecture

```text
Psychology PDFs / CSVs
        â†“
Text Chunking
        â†“
Embeddings (HuggingFace / Ollama)
        â†“
Chroma Vector Database
        â†“
Relevance Filtering
        â†“
RAG Prompt + Safety Guardrails
        â†“
LLM (Mistral via Ollama)
        â†“
User (CLI / Streamlit UI)
````

---

## ğŸ› ï¸ Tech Stack

* **Language:** Python
* **LLM:** Mistral (via Ollama)
* **Frameworks:** LangChain, Streamlit
* **Vector Database:** ChromaDB
* **Embeddings:**

  * HuggingFace Sentence Transformers
  * Ollama embeddings (fallback)
* **OCR (optional):** Tesseract, pdf2image

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/RAG-Based-Psychology-Mentor-AI.git
cd RAG-Based-Psychology-Mentor-AI
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate   # Linux / macOS
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run Ollama (Required)

Ensure Ollama is running locally and Mistral is pulled:

```bash
ollama pull mistral
```

---

## ğŸ“š Data Ingestion (RAG Setup)

Place your psychology PDFs / CSVs inside the `data/` directory, then run:

```bash
python populate_dataset.py --reset
```

This will:

* Load documents
* Chunk text
* Generate embeddings
* Store them in ChromaDB

---

## ğŸ’¬ Running the Chatbot

### â–¶ï¸ CLI Mode

```bash
python query_data.py
```

### ğŸŒ Streamlit Web App

```bash
streamlit run streamlit_app.py
```

---

## ğŸ§ª Testing & Evaluation

The project includes **behavioral tests** to evaluate:

* Empathy tone
* Boundary adherence
* Non-diagnostic responses

Run example test:

```bash
python test_empathy_tone.py
```

These tests focus on **response quality**, not exact string matching.

---

## ğŸ›¡ï¸ Safety & Ethical Design

This system intentionally includes safeguards:

* âŒ No diagnosis or medical claims
* âŒ No therapy or treatment advice
* âŒ No emotional dependency encouragement
* âœ… Crisis keyword interception with human-resource redirection
* âœ… Context filtering to avoid irrelevant or harmful retrieval

See **`ETHICAL_CONSIDERATIONS.md`** for detailed discussion.

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ query_data.py              # CLI chatbot with RAG + safety guards
â”œâ”€â”€ streamlit_app.py           # Web UI
â”œâ”€â”€ populate_dataset.py        # Data ingestion & vector store setup
â”œâ”€â”€ get_embedding_function.py  # Embedding provider abstraction
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_empathy_tone.py   # Empathy & boundary evaluation
â”œâ”€â”€ data/                      # Input documents (not tracked)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## Evaluation Results

The system was evaluated through scenario-based qualitative testing focused on empathy, boundary adherence, and safety behavior rather than exact response matching. Test prompts included common student concerns (e.g., loneliness, academic failure), psychology concept queries, diagnostic-seeking questions, and crisis-related statements. Across these scenarios, the chatbot consistently demonstrated empathetic, non-judgmental tone, accurately translated psychology concepts into accessible language, and refused diagnostic or medical requests while appropriately redirecting users to professional resources. In crisis-oriented prompts, the system reliably interrupted normal conversation flow and escalated responses toward human support mechanisms. These results indicate that the system effectively balances emotional support with ethical constraints, supporting its intended role as a non-clinical psychology mentor rather than a therapeutic agent.


---

## ğŸ”® Future Improvements

* Semantic (ML-based) crisis detection
* Emotion intensity scaling
* Long-term memory summarization
* Multilingual support
* Human-in-the-loop review mode

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
You are free to use, modify, and distribute it with attribution.

---

## ğŸ‘¤ Author

Developed by **Kritansh Uppal**
For educational, research, and portfolio purposes.

---

â­ If you find this project useful or interesting, consider starring the repository.

```


